2025-06-16 04:01:48,790 - ============================== args ==============================
2025-06-16 04:01:48,790 - dataset: weibo
2025-06-16 04:01:48,790 - model: Scheduler
2025-06-16 04:01:48,790 - batch: 32
2025-06-16 04:01:48,790 - mode: both
2025-06-16 04:01:48,790 - seed: 2024
2025-06-16 04:01:48,790 - ============================== End args ==============================
2025-06-16 04:01:48,790 - ============================== config ==============================
2025-06-16 04:01:48,790 - BertModel: <class 'transformers.models.bert.modeling_bert.BertModel'>
2025-06-16 04:01:48,790 - BertTokenizer: <class 'transformers.models.bert.tokenization_bert.BertTokenizer'>
2025-06-16 04:01:48,790 - CLIPModel: <class 'transformers.models.clip.modeling_clip.CLIPModel'>
2025-06-16 04:01:48,790 - CLIPProcessor: <class 'transformers.models.clip.processing_clip.CLIPProcessor'>
2025-06-16 04:01:48,790 - att_dropout: 0
2025-06-16 04:01:48,790 - att_num_heads: 8
2025-06-16 04:01:48,790 - batch_size: 32
2025-06-16 04:01:48,790 - bert_dir: ./bert-base-multilingual-uncased/
2025-06-16 04:01:48,790 - classifier_hidden_dim: 128
2025-06-16 04:01:48,790 - clip_dir: ./clip-vit-base-patch32/
2025-06-16 04:01:48,790 - decayRate: 0.96
2025-06-16 04:01:48,790 - device: cuda
2025-06-16 04:01:48,790 - edge_feats: 768
2025-06-16 04:01:48,790 - epoch: 10
2025-06-16 04:01:48,790 - f_dropout: 0
2025-06-16 04:01:48,790 - hidden_dim: 768
2025-06-16 04:01:48,790 - img_dim: 768
2025-06-16 04:01:48,791 - knowledge_enhanced: True
2025-06-16 04:01:48,791 - lr: 5e-05
2025-06-16 04:01:48,791 - max_captions_num: 5
2025-06-16 04:01:48,791 - max_images_num: 5
2025-06-16 04:01:48,791 - model_saved_path: ./checkpoints/
2025-06-16 04:01:48,791 - n_layers: 2
2025-06-16 04:01:48,791 - node_feats: 768
2025-06-16 04:01:48,791 - num_classes: 3
2025-06-16 04:01:48,791 - num_heads: 2
2025-06-16 04:01:48,791 - out_feats: 768
2025-06-16 04:01:48,791 - patience: 3
2025-06-16 04:01:48,791 - pheme_dataset_dir: ../data/PHEME/
2025-06-16 04:01:48,791 - scheduler_bert_freeze: True
2025-06-16 04:01:48,791 - scheduler_swintransformer_freeze: True
2025-06-16 04:01:48,791 - swin_transformer: ./swin-transformer
2025-06-16 04:01:48,791 - sys1_bert_freeze: True
2025-06-16 04:01:48,791 - sys1_clip_freeze: True
2025-06-16 04:01:48,791 - sys1_swintransformer_freeze: True
2025-06-16 04:01:48,791 - sys2_bert_freeze: False
2025-06-16 04:01:48,791 - sys2_clip_freeze: False
2025-06-16 04:01:48,791 - sys2_swintransformer_freeze: False
2025-06-16 04:01:48,791 - test_ratio: 0.1
2025-06-16 04:01:48,791 - text_dim: 768
2025-06-16 04:01:48,791 - text_max_length: 40
2025-06-16 04:01:48,791 - torch: <module 'torch' from '/home/huang001/anaconda3/lib/python3.11/site-packages/torch/__init__.py'>
2025-06-16 04:01:48,791 - train_ratio: 0.8
2025-06-16 04:01:48,791 - twitter_dataset_dir: ../data/Twitter/
2025-06-16 04:01:48,791 - val_ratio: 0.1
2025-06-16 04:01:48,791 - weibo2_dataset_dir: ../data/Weibo+/
2025-06-16 04:01:48,791 - weibo_dataset_dir: ../data/Weibo/
2025-06-16 04:01:48,791 - ============================== End config ==============================
2025-06-16 04:01:49,470 - total number of parameters:196994
2025-06-16 04:01:49,473 - -----------Epoch:0-----------
2025-06-16 04:01:51,602 - epoch: 0, step: 0, loss: 0.6906, acc: 0.3125
2025-06-16 04:02:11,814 - epoch: 0, step: 10, loss: 0.6999, acc: 0.1562
2025-06-16 04:02:31,714 - epoch: 0, step: 20, loss: 0.7488, acc: 0.1562
2025-06-16 04:02:49,533 - epoch: 0, step: 30, loss: 0.6939, acc: 0.3750
2025-06-16 04:03:07,717 - epoch: 0, step: 40, loss: 0.7159, acc: 0.4062
2025-06-16 04:03:25,831 - epoch: 0, step: 50, loss: 0.6811, acc: 0.7188
2025-06-16 04:03:43,752 - epoch: 0, step: 60, loss: 0.6545, acc: 0.7188
2025-06-16 04:04:03,548 - epoch: 0, step: 70, loss: 0.7134, acc: 0.7188
2025-06-16 04:04:19,455 - epoch: 0, step: 80, loss: 0.6536, acc: 0.5938
2025-06-16 04:04:35,766 - epoch: 0, step: 90, loss: 0.6780, acc: 0.2188
2025-06-16 04:04:51,760 - epoch: 0, step: 100, loss: 0.6240, acc: 0.2812
2025-06-16 04:05:08,846 - epoch: 0, step: 110, loss: 0.7026, acc: 0.2500
2025-06-16 04:05:28,642 - epoch: 0, step: 120, loss: 0.6632, acc: 0.3438
2025-06-16 04:05:46,449 - epoch: 0, step: 130, loss: 0.6493, acc: 0.4688
2025-06-16 04:06:03,275 - epoch: 0, step: 140, loss: 0.6652, acc: 0.3125
2025-06-16 04:06:20,069 - epoch: 0, step: 150, loss: 0.6041, acc: 0.4062
2025-06-16 04:06:35,129 - epoch: 0, step: 160, loss: 0.6991, acc: 0.2500
2025-06-16 04:06:55,624 - epoch: 0, step: 170, loss: 0.6850, acc: 0.3750
2025-06-16 04:07:11,741 - epoch: 0, step: 180, loss: 0.6763, acc: 0.3750
2025-06-16 04:07:28,897 - epoch: 0, step: 190, loss: 0.7333, acc: 0.4062
2025-06-16 04:07:31,242 - train_loss:0.67497 train_acc:0.387
2025-06-16 04:08:14,215 - val_loss:0.65391 val_acc:0.506 

2025-06-16 04:08:15,334 - save model,acc:0.506
2025-06-16 04:08:15,334 - -----------Epoch:1-----------
2025-06-16 04:08:17,712 - epoch: 1, step: 0, loss: 0.7145, acc: 0.4688
2025-06-16 04:08:35,700 - epoch: 1, step: 10, loss: 0.6320, acc: 0.5625
2025-06-16 04:08:53,627 - epoch: 1, step: 20, loss: 0.6348, acc: 0.6875
2025-06-16 04:09:11,857 - epoch: 1, step: 30, loss: 0.6423, acc: 0.6250
2025-06-16 04:09:31,329 - epoch: 1, step: 40, loss: 0.6678, acc: 0.5938
2025-06-16 04:09:50,230 - epoch: 1, step: 50, loss: 0.6685, acc: 0.3750
2025-06-16 04:10:06,844 - epoch: 1, step: 60, loss: 0.6486, acc: 0.4688
2025-06-16 04:10:20,866 - epoch: 1, step: 70, loss: 0.6643, acc: 0.6250
2025-06-16 04:10:39,457 - epoch: 1, step: 80, loss: 0.6332, acc: 0.4688
2025-06-16 04:10:54,872 - epoch: 1, step: 90, loss: 0.6732, acc: 0.3750
2025-06-16 04:11:14,994 - epoch: 1, step: 100, loss: 0.6506, acc: 0.5312
2025-06-16 04:11:33,936 - epoch: 1, step: 110, loss: 0.6228, acc: 0.4375
2025-06-16 04:11:54,025 - epoch: 1, step: 120, loss: 0.5941, acc: 0.6875
2025-06-16 04:12:07,975 - epoch: 1, step: 130, loss: 0.5740, acc: 0.5938
2025-06-16 04:12:24,466 - epoch: 1, step: 140, loss: 0.6573, acc: 0.4688
2025-06-16 04:12:39,698 - epoch: 1, step: 150, loss: 0.6512, acc: 0.2500
2025-06-16 04:13:01,442 - epoch: 1, step: 160, loss: 0.8078, acc: 0.1562
2025-06-16 04:13:17,085 - epoch: 1, step: 170, loss: 0.5886, acc: 0.4688
2025-06-16 04:13:34,998 - epoch: 1, step: 180, loss: 0.6301, acc: 0.4688
2025-06-16 04:13:55,439 - epoch: 1, step: 190, loss: 0.6760, acc: 0.5000
2025-06-16 04:13:58,753 - train_loss:0.64400 train_acc:0.500
2025-06-16 04:14:40,159 - val_loss:0.63064 val_acc:0.506 

2025-06-16 04:14:40,160 - -----------Epoch:2-----------
2025-06-16 04:14:41,729 - epoch: 2, step: 0, loss: 0.5718, acc: 0.5312
2025-06-16 04:14:57,791 - epoch: 2, step: 10, loss: 0.7343, acc: 0.4688
2025-06-16 04:15:14,919 - epoch: 2, step: 20, loss: 0.6693, acc: 0.4688
2025-06-16 04:15:33,099 - epoch: 2, step: 30, loss: 0.6271, acc: 0.5312
2025-06-16 04:15:51,519 - epoch: 2, step: 40, loss: 0.6618, acc: 0.5000
2025-06-16 04:16:07,968 - epoch: 2, step: 50, loss: 0.5790, acc: 0.5938
2025-06-16 04:16:26,066 - epoch: 2, step: 60, loss: 0.5720, acc: 0.5000
2025-06-16 04:16:44,195 - epoch: 2, step: 70, loss: 0.7168, acc: 0.2812
2025-06-16 04:17:00,554 - epoch: 2, step: 80, loss: 0.9150, acc: 0.5625
2025-06-16 04:17:17,230 - epoch: 2, step: 90, loss: 0.5651, acc: 0.5938
2025-06-16 04:17:34,452 - epoch: 2, step: 100, loss: 0.5937, acc: 0.6562
2025-06-16 04:17:53,440 - epoch: 2, step: 110, loss: 0.6477, acc: 0.3125
2025-06-16 04:18:10,246 - epoch: 2, step: 120, loss: 0.6130, acc: 0.5000
2025-06-16 04:18:27,408 - epoch: 2, step: 130, loss: 0.5212, acc: 0.4375
2025-06-16 04:18:46,350 - epoch: 2, step: 140, loss: 0.6991, acc: 0.4688
2025-06-16 04:19:06,344 - epoch: 2, step: 150, loss: 0.7242, acc: 0.5312
2025-06-16 04:19:24,886 - epoch: 2, step: 160, loss: 0.5943, acc: 0.5312
2025-06-16 04:19:44,493 - epoch: 2, step: 170, loss: 0.5989, acc: 0.6250
2025-06-16 04:20:01,654 - epoch: 2, step: 180, loss: 0.7417, acc: 0.3750
2025-06-16 04:20:20,200 - epoch: 2, step: 190, loss: 0.6187, acc: 0.6250
2025-06-16 04:20:23,322 - train_loss:0.62027 train_acc:0.523
2025-06-16 04:21:05,042 - val_loss:0.62400 val_acc:0.589 

2025-06-16 04:21:06,549 - save model,acc:0.589
2025-06-16 04:21:06,549 - -----------Epoch:3-----------
2025-06-16 04:21:08,314 - epoch: 3, step: 0, loss: 0.5826, acc: 0.6875
2025-06-16 04:21:25,857 - epoch: 3, step: 10, loss: 0.5704, acc: 0.7188
2025-06-16 04:21:43,412 - epoch: 3, step: 20, loss: 0.6377, acc: 0.6875
2025-06-16 04:21:58,566 - epoch: 3, step: 30, loss: 0.7527, acc: 0.5625
2025-06-16 04:22:16,188 - epoch: 3, step: 40, loss: 0.5142, acc: 0.5625
2025-06-16 04:22:31,517 - epoch: 3, step: 50, loss: 0.5461, acc: 0.7188
2025-06-16 04:22:49,844 - epoch: 3, step: 60, loss: 0.5732, acc: 0.7188
2025-06-16 04:23:09,047 - epoch: 3, step: 70, loss: 0.6192, acc: 0.5938
2025-06-16 04:23:30,170 - epoch: 3, step: 80, loss: 0.5059, acc: 0.6875
2025-06-16 04:23:45,443 - epoch: 3, step: 90, loss: 0.6683, acc: 0.5625
2025-06-16 04:24:02,407 - epoch: 3, step: 100, loss: 0.7566, acc: 0.5625
2025-06-16 04:24:20,949 - epoch: 3, step: 110, loss: 0.6951, acc: 0.5625
2025-06-16 04:24:40,647 - epoch: 3, step: 120, loss: 0.5721, acc: 0.5312
2025-06-16 04:24:58,132 - epoch: 3, step: 130, loss: 0.6808, acc: 0.4688
2025-06-16 04:25:14,761 - epoch: 3, step: 140, loss: 0.6568, acc: 0.3438
2025-06-16 04:25:33,140 - epoch: 3, step: 150, loss: 0.6707, acc: 0.4375
2025-06-16 04:25:52,268 - epoch: 3, step: 160, loss: 0.6682, acc: 0.5312
2025-06-16 04:26:10,178 - epoch: 3, step: 170, loss: 0.6926, acc: 0.5000
2025-06-16 04:26:30,522 - epoch: 3, step: 180, loss: 0.5654, acc: 0.8438
2025-06-16 04:26:48,163 - epoch: 3, step: 190, loss: 0.5904, acc: 0.6875
2025-06-16 04:26:50,597 - train_loss:0.60745 train_acc:0.588
2025-06-16 04:27:32,223 - val_loss:0.60722 val_acc:0.519 

2025-06-16 04:27:32,223 - -----------Epoch:4-----------
2025-06-16 04:27:33,634 - epoch: 4, step: 0, loss: 0.6223, acc: 0.5312
2025-06-16 04:27:48,711 - epoch: 4, step: 10, loss: 0.5785, acc: 0.5312
2025-06-16 04:28:07,851 - epoch: 4, step: 20, loss: 0.5579, acc: 0.5625
2025-06-16 04:28:25,325 - epoch: 4, step: 30, loss: 0.5803, acc: 0.7188
2025-06-16 04:28:43,838 - epoch: 4, step: 40, loss: 0.6266, acc: 0.6875
2025-06-16 04:28:59,796 - epoch: 4, step: 50, loss: 0.5950, acc: 0.7812
2025-06-16 04:29:16,128 - epoch: 4, step: 60, loss: 0.6168, acc: 0.7500
2025-06-16 04:29:35,803 - epoch: 4, step: 70, loss: 0.5715, acc: 0.5312
2025-06-16 04:29:54,249 - epoch: 4, step: 80, loss: 0.7670, acc: 0.4062
2025-06-16 04:30:15,283 - epoch: 4, step: 90, loss: 0.6387, acc: 0.5000
2025-06-16 04:30:35,487 - epoch: 4, step: 100, loss: 0.5239, acc: 0.5938
2025-06-16 04:30:51,452 - epoch: 4, step: 110, loss: 0.6391, acc: 0.5000
2025-06-16 04:31:09,797 - epoch: 4, step: 120, loss: 0.6022, acc: 0.6250
2025-06-16 04:31:27,138 - epoch: 4, step: 130, loss: 0.5926, acc: 0.6875
2025-06-16 04:31:42,128 - epoch: 4, step: 140, loss: 0.4891, acc: 0.7188
2025-06-16 04:32:01,185 - epoch: 4, step: 150, loss: 0.6176, acc: 0.4375
2025-06-16 04:32:19,159 - epoch: 4, step: 160, loss: 0.6236, acc: 0.4375
2025-06-16 04:32:38,046 - epoch: 4, step: 170, loss: 0.7549, acc: 0.4062
2025-06-16 04:32:59,114 - epoch: 4, step: 180, loss: 0.6005, acc: 0.6250
2025-06-16 04:33:14,819 - epoch: 4, step: 190, loss: 0.5374, acc: 0.7500
2025-06-16 04:33:16,926 - train_loss:0.59467 train_acc:0.577
2025-06-16 04:33:58,636 - val_loss:0.61001 val_acc:0.572 

2025-06-16 04:33:58,637 - -----------Epoch:5-----------
2025-06-16 04:34:00,959 - epoch: 5, step: 0, loss: 0.5482, acc: 0.5000
2025-06-16 04:34:18,140 - epoch: 5, step: 10, loss: 0.5381, acc: 0.7500
2025-06-16 04:34:36,131 - epoch: 5, step: 20, loss: 0.5269, acc: 0.8438
2025-06-16 04:34:53,917 - epoch: 5, step: 30, loss: 0.4885, acc: 0.5625
2025-06-16 04:35:13,102 - epoch: 5, step: 40, loss: 0.5596, acc: 0.5625
2025-06-16 04:35:31,144 - epoch: 5, step: 50, loss: 0.6086, acc: 0.5312
2025-06-16 04:35:51,408 - epoch: 5, step: 60, loss: 0.4533, acc: 0.7188
2025-06-16 04:36:08,356 - epoch: 5, step: 70, loss: 0.5394, acc: 0.5938
2025-06-16 04:36:24,798 - epoch: 5, step: 80, loss: 0.6546, acc: 0.6562
2025-06-16 04:36:41,161 - epoch: 5, step: 90, loss: 0.5968, acc: 0.5625
2025-06-16 04:36:59,530 - epoch: 5, step: 100, loss: 0.6865, acc: 0.5938
2025-06-16 04:37:16,587 - epoch: 5, step: 110, loss: 0.5435, acc: 0.7812
2025-06-16 04:37:34,657 - epoch: 5, step: 120, loss: 0.7650, acc: 0.7188
2025-06-16 04:37:52,786 - epoch: 5, step: 130, loss: 0.6094, acc: 0.5625
2025-06-16 04:38:12,134 - epoch: 5, step: 140, loss: 0.5677, acc: 0.5000
2025-06-16 04:38:31,452 - epoch: 5, step: 150, loss: 0.6178, acc: 0.5000
2025-06-16 04:38:48,876 - epoch: 5, step: 160, loss: 0.6413, acc: 0.5312
2025-06-16 04:39:05,853 - epoch: 5, step: 170, loss: 0.5800, acc: 0.5938
2025-06-16 04:39:25,532 - epoch: 5, step: 180, loss: 0.6821, acc: 0.3750
2025-06-16 04:39:40,854 - epoch: 5, step: 190, loss: 0.5613, acc: 0.5000
2025-06-16 04:39:43,924 - train_loss:0.58099 train_acc:0.608
2025-06-16 04:40:24,752 - val_loss:0.59330 val_acc:0.521 

2025-06-16 04:41:07,890 - --------------------- test results-------------------------------
2025-06-16 04:41:07,891 - acc:0.59792477  prec:[0.92470586 0.19653179]  rec:[0.585693 0.68    ]  f1:[0.7171533  0.30493274]
2025-06-16 04:41:07,891 - confusion: 
[[393 278]
 [ 32  68]]
2025-06-16 04:41:07,891 - the running time is: 2358.4 s
